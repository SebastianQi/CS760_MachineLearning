%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CS 760 Homework 5:\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Qihong Lu} % Your name
\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

\section*{Question1}
\textbf{
Show a neural network that represents the logical function $\mathbf{y = (x1 \land x2) \lor (x3 \land x4)}$. Specifically, show the network topology, weights and biases. You should assume that hidden and output units use sigmoid output functions, and an output-unit activation of 0.5 or greater represents a true prediction for y.\\ 
}

The network architecture is shown below. The weights are denoted in black. $X_i$ are the input units. $h_i$ are the hidden units. $b_i$ are the bias units. Finally, there is a single output unit. 

\begin{center}
	\includegraphics[scale=.4]{pics/hw5-1.png}
\end{center}


The unit h1 is representing an AND function over X1 and X2. 
Similarly, the unit h2 is representing an AND function over X3 and X4. Finally, the output unit represents an OR function over h1 and h2. 

\newpage
\section*{Question2}
\textbf{Consider the concept class C in which each concept is an interval on the line of real numbers. Each training instance is represented by a single real-valued feature x, and a binary class label \boldmath$y \in \{0, 1\}$. A learned concept is represented by an interval [a, a + b] where a is real value and b is a positive real value, and the concept predicts y=1 for values of x in the interval, and y=0 otherwise. Show that C is PAC learnable.}
\bigbreak

The hypothesis space, H consists of all possible intervals in the form of [a, a + b]. The VC-dimension(H), or the max number of the training set can be shattered by H, is 2. Because when there are 3 instances: $(x_1,y_1),(x_2,y_2),(x_3,y_3)$ with $x_1< x_2< x_3$ and $y_1 = y_3 = 1 $and $ y_2 = 0$, there is no $h \in H$ that can achieve zero training error. 
\bigbreak
Because we have a continuous hypothesis space, we can use the following bound for the sample complexity: 
$$
m \geq \frac{1}{\epsilon} (4 \text{log}_2 \frac{2}{\delta} + 8 \text{VC-dim}(H) \text{log}_2 \frac{13}{\epsilon}) = \frac{1}{\epsilon} (4 \text{log}_2 \frac{2}{\delta} + 16 \text{log}_2 \frac{13}{\epsilon})
$$

Therefore, m grows polynomially in $\frac{1}{\epsilon}$, $\frac{1}{\delta}$. 

\bigbreak
I still need to show that we can find a consistent hypothesis in polynomial time. 

Given a training set $D = \{x_i,y_i\}$, i = 1,2,..., N. 

Let $c = y_1 \in {0,1}$. Loop over $y_i$ from 1 to m. When $y_i \neq c$, set a = $x_i$, c = $y_i$, t = i. Keep looping over value of $y_i$, start from i = t. When $y_i \neq c$, set b = $x_i$ - a. 
\bigbreak 
This algorithm will find a consistent hypothesis (assuming consistent hypothesis exists) in linear time w.r.t to the number of training example. Therefore, C is PAC learnable. 

\newpage
\section*{Question3}
\textbf{Consider the concept class that consists of disjunctions of exactly two literals where each literal is a feature or its negation, and at most one literal can be negated. Suppose that the number of features n = 3. Show what the Halving algorithm would do with the following two training instances in an on-line setting. Specifically, show the initial version space, the prediction made by the Halving algorithm for each instance, and the resulting version space after receiving the label of each instance.}

\begin{center}
\begin{tabular}{ c c c c}
 x1 & x2 & x3 & y \\ 
 T & F & F & pos \\  
 F & T & T & neg \\  
\end{tabular}
\end{center}

At time t, given the training instance $x^{(t)}$, the Halving algorithm reduces the version space, $VS_t$, by remove all hypotheses, $h \in H$, that are inconsistance with $x^{(t)}$. 
\bigbreak

Initiailly, $VS_0 = 
\{
x_1 \lor x_2, x_1 \lor x_3, x_2 \lor x_3,
x_1 \lor \neg x_2, x_1 \lor \neg x_3, x_2 \lor \neg x_3,
\neg x_1 \lor x_2, \neg x_1 \lor x_3, \neg x_2 \lor x_3
\}$
, where element in the version space in the form of $x_i \lor x_j$ represents a hypothesis that predicts positive when $x_i \lor x_j$ is satisfied. 
\bigbreak
Step 1: Given the first instance $[x_1,x_2,x_3] = [T,F,F]$ with y = Positive. The majority vote of $VS_0$ is Positive. The predictions of the inidividual hypothesis are listed here: 

\begin{center}
$
\begin{array}{c|c|c}
\hline
\text{hypothesis} & \text{prediction} & \text{remove?} \\
\hline
x_1 \lor x_2 & \text{Postive} &  \\ 
x_1 \lor x_3 & \text{Postive} & \\ 
x_2 \lor x_3 & \text{Negative} & T \\ 
x_1 \lor \neg x_2 & \text{Postive} &  \\ 
x_1 \lor \neg x_3 & \text{Postive} &  \\ 
x_2 \lor \neg x_3 & \text{Postive} &  \\ 
\neg x_1 \lor x_2 & \text{Negative} & T \\ 
\neg x_1 \lor x_3 & \text{Negative} & T \\ 
\neg x_2 \lor x_3 & \text{Postive} &  \\ 
\hline
\end{array}
$
\end{center}

Update $VS_1 = 
\{
x_1 \lor x_2, x_1 \lor x_3, 
x_1 \lor \neg x_2, x_1 \lor \neg x_3, x_2 \lor \neg x_3,
\neg x_2 \lor x_3
\}$

\smallbreak
Step 2: Given the 2nd instance $[x_1,x_2,x_3] = [F,T,T]$ with y = Negative. The majority vote $VS_1$ is Positive. The predictions of the inidividual hypothesis are listed here: 
\begin{center}
$
\begin{array}{c|c|c}
\hline
\text{hypothesis} & \text{prediction} & \text{remove?} \\
\hline
x_1 \lor x_2 & \text{Postive} & T \\ 
x_1 \lor x_3 & \text{Postive} & T \\ 
x_1 \lor \neg x_2 & \text{Negative} &  \\ 
x_1 \lor \neg x_3 & \text{Negative} &  \\ 
x_2 \lor \neg x_3 & \text{Postive} & T \\ 
\neg x_2 \lor x_3 & \text{Postive} & T \\ 
\hline
\end{array}
$
\end{center}
Update $VS_2 = 
\{
x_1 \lor \neg x_2, x_1 \lor \neg x_3
\}$


\newpage
\section*{Question4}
\textbf{In this same setting, suppose the learner can pick the next training instance it will be given. That is, the learner can pick the feature vector part of the instance; the class label will be provided by the teacher. Which instance should it ask for next? Justify your answer.}
\bigbreak 
After the 2nd step, the version space is $VS_2 = 
\{
x_1 \lor \neg x_2, x_1 \lor \neg x_3
\}$
\smallbreak
I think asking the y value of the instance $[x_1,x_2,x_3] = [F,T,F]$ or $[x_1,x_2,x_3] = [F,F,T]$ is informative (there are other instances with the same effect). Because one hypothesis will predict Postive and the other one will predict Negative. 
\bigbreak
Take the instance $[x_1,x_2,x_3] = [F,F,T]$ for example, here are the predictions generated by $VS_2$: 

\begin{center}
$
\begin{array}{c|c}
\hline
\text{hypothesis} & \text{prediction} \\
\hline
x_1 \lor \neg x_2 & \text{Positive}  \\ 
x_1 \lor \neg x_3 & \text{Negative}  \\ 
\hline
\end{array}
$
\end{center}

In this case, no matter what the true label is, we can reduce the current size of the Version space by a half, since the two hypotheses disagree.  

\newpage
\section*{Question5}
\textbf{How many mistakes will the Halving algorithm make for this concept class in the worst case? Justify your answer.}
\bigbreak 


By assumption, the concept class that consists of disjunctions of exactly two literals out of n features. And each literal is a feature or its negation, and at most one literal can be negated. Therefore, there are 
$
{{n}\choose{2}} \times 3 = \frac{3}{2}(n-1)n
$
possible hypotheses. 
\smallbreak
In the worsest case, assume the Halving algorithm always make mistakes. Namely, for each $x_i$, majority of the hypotheses are inconsistent with $x_i$. Then we know that at each iteration, the hypotheses space shrink by at least $\frac{1}{2}$. So the havling algorithm will make at most  $\floor*{\text{log}_2 |H|}$ mistakes. Plug in the size of |H|, I get $\floor*{\text{log}_2 \frac{3}{2}(n-1)n}$. 
\bigbreak

For our case when n = 3. $\floor*{\text{log}_2 \frac{3}{2}(n-1)n} = \floor*{\text{log}_2 9} = \floor*{3.1699 \cdots} = 3$


\newpage
\section*{Question6}
\textbf{Given the initial Bayes net parameters and training set depicted below, show how the network parameters would be updated after one step of the EM procedure. The `?' symbol indicates that the value for the variable A is missing in a given training instance.}
\begin{center}
	\includegraphics[scale=.4]{pics/hw5_6.png}
\end{center}
\bigbreak

\textbf{E-step}: fill in the missing values in the A column 
\smallbreak
For the 1st missing instance: 
\begin{flalign*} 
P(a | b, c) &= \frac{P(a,b,c)}{P(a,b,c) + P(\neg a,b,c)} = \frac{P(a)P(b)P(c|a,b)}{P(a)P(b)P(c|a,b) + P(\neg a)P(b)P(c|\neg a,b)} \\
&= \frac{.7 \times .5 \times .8}{.7 \times .5 \times .8 + .3 \times .5 \times .3} \approx .86 \\ 
P(\neg a | b, c) &= 1 - P(a | b, c) \approx .14
\end{flalign*}

\smallbreak
For the 5th instance, which is also missing: 
\begin{flalign*} 
P(a|\neg b,c) &= \frac{P(a, \neg b,c)}{P(a, \neg b,c) + P(\neg a, \neg b,c)} = \frac{P(a)P(\neg b)P(c|a,\neg b)}{P(a)P(\neg b)P(c|a,\neg b) + P(\neg a)P( \neg b)P(c|\neg a,\neg b)} \\
&= \frac{.7 \times .5 \times .4}{.7 \times .5 \times .4 + .3 \times .5 \times .9} \approx .51 \\ 
P(\neg a|\neg b,c) &= 1 - P(a|\neg b,c) \approx .49
\end{flalign*}

\bigbreak

\textbf{M-step}: re-estimate the conditional probability table using expected counts 
\begin{flalign*} 
P(a) &=   \frac{4 + .86 + .51}{8} = .67125\\ 
P(c | a, b) &=  \frac{E \# (c, a, b)}{E \# (a, b)} = \frac{.86 + 1}{2 + .86} \approx .65\\ 
P(c | \neg a, b) &=  \frac{E \# (c, \neg a, b)}{E \# (\neg a, b)} = \frac{.14 }{2 + .14} \approx .065\\
P(c | a, \neg b) &=  \frac{E \# (c, a, \neg b)}{E \# (a, \neg b)} = \frac{.51 + 1}{2 + .51} \approx .60\\
P(c | \neg a, \neg b) &= \frac{E \# (c, \neg a, \neg b)}{E \# (\neg a, \neg b)} = \frac{.49}{1 + .49} \approx 33\\
\end{flalign*}

\newpage
\section*{Question7}
\textbf{Consider a learning task in which you are given n features and you want to use a feature selection method along with your learning algorithm. Specifically, suppose you are using forward selection along with k-fold cross validation to evaluate each feature set during the search process. Assume that there are r relevant features, and forward selection stops after selecting r features. Given a single training set, how many models are learned in the process of finding a feature subset of size r?}
\bigbreak

For forward selection, we begin with the null model (the model with no feature) and add one informative feature at each iteration, where the informativeness of a given feature is determined based on the cross-validated performance. 
\bigbreak
At the 1st iteration, there are n candidate features, I need to evaluate n models. \\
At the 2nd iteration, there are n-1 candidate features, I need to evaluate n-1 models. \\
......\\
At the r-th iteration, there are n-r candidate features, I need to evaluate n-r models. ($r \leq n$)
\bigbreak

Therefore, to choose r features, we need to evaluate $n + (n-1) + \cdots + (n-r) $ models. \\

When evaluating each model, a k-folds cross validation is required. So in total, we need to fit $k(n + (n-1) + \cdots + (n-r))$ models. 

\newpage
\section*{Question8}
\textbf{Now suppose instead you are using backward elimination for the same task. Again, assume that the search process stops after selecting >r features and does not consider feature subsets smaller than this. Given a single training set, how many models are learned in the process of finding a feature subset of size r?}
\bigbreak
For backward elimination, we begin with the full model (the model with all features) and remove the least informative feature at each iteration, where the informativeness of a given feature is determined based on the cross-validated performance. 
\bigbreak
1st iteration: there are n possible features can be removed, need to evaluate n models. \\
2nd iteration: there are n-1 possible features can be removed, need to evaluate n-1 models. \\
......\\
(n-r)-th iteration: there are n-r possible features can be removed, need to evaluate n-(n-r) = r models. ($r \leq n$)
\bigbreak

Therefore, to remove (n-r) features, we need to evaluate $n + (n-1) + \cdots + (n-(n-r))$ models. \\

When evaluating each model, a k-folds cross validation is required. So in total, we need to fit $k(n + (n-1) + \cdots + (n-(n-r)))$ models. 



\newpage
\section*{Question9}
\textbf{Consider the relational learning task defined below. List all of the literals that would be considered by FOIL algorithm on the first step of leaning a rule for the aunt(X, Y) relation. }
\begin{center}
	\includegraphics[scale=.45]{pics/hw5_9.png}
\end{center}

Here're are the literals that would be considered, for the relation $\text{aunt}(X,Y)$, on the 1st step: 
\smallbreak

$X == Y$	\\
$X \neq Y$ \\ 
$\text{sibling}(X,Y)$ \\ 
$\neg \text{sibling}(X,Y)$ \\ 
$\text{parent}(X,Y)$ \\ 
$\neg \text{parent}(X,Y)$ \\ 
$\text{female}(X)$ \\ 
$\neg \text{female}(X)$ \\ 
$\text{female}(Y)$ \\ 
$\neg \text{female}(Y)$ \\ 
$\text{male}(X)$ \\ 
$\neg \text{male}(X)$ \\ 
$\text{male}(Y)$ \\ 
$\neg \text{male}(Y)$ \\ 
$X == c, \text{where c is a constant}$ \\ 
$X \neq c, \text{where c is a constant}$ \\ 
$Y == c, \text{where c is a constant}$ \\ 
$Y \neq c, \text{where c is a constant}$ 







\iffalse
$\text{aunt}(X,Y) \equiv X == Y$	\\
$\text{aunt}(X,Y) \equiv X \neq Y$ \\ 
$\text{aunt}(X,Y) \equiv \text{sibling}(X,Y)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{sibling}(X,Y)$ \\ 
$\text{aunt}(X,Y) \equiv \text{parent}(X,Y)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{parent}(X,Y)$ \\ 
$\text{aunt}(X,Y) \equiv \text{female}(X)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{female}(X)$ \\ 
$\text{aunt}(X,Y) \equiv \text{female}(Y)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{female}(Y)$ \\ 
$\text{aunt}(X,Y) \equiv \text{male}(X)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{male}(X)$ \\ 
$\text{aunt}(X,Y) \equiv \text{male}(Y)$ \\ 
$\text{aunt}(X,Y) \equiv \neg \text{male}(Y)$ \\ 
$\text{aunt}(X,Y) \equiv X == c, \text{where c is a constant}$ \\ 
$\text{aunt}(X,Y) \equiv X \neq c, \text{where c is a constant}$ \\ 
$\text{aunt}(X,Y) \equiv Y == c, \text{where c is a constant}$ \\ 
$\text{aunt}(X,Y) \equiv Y \neq c, \text{where c is a constant}$ 
\fi 





\newpage
\section*{Question10}
\textbf{Show the FOIL\_gain calculation when sibling(Y, Z) is considered as the first literal to be added to the first rule learning by FOIL. Also show the tuples that are involved in this calculation.}

Consider aunt(X, Y) <- sibling(Y, Z)
Now we can expand all the tuples: 

\begin{tabular}{ c c c c }
Positive Tuples: & & & \\ 
 & <o,t> & -> & <o,t,s>, <o,t,d> \\  
 & <y,t> & -> & <y,t,s>, <y,t,o> \\   
 & <a,s> & -> & <a,s,t>, <a,s,d> \\
 & <b,s> & -> & <a,s,t>, <a,s,d> 
\end{tabular}

\begin{tabular}{ c c c c }
Negative Tuples: & & & \\ 
 & <o,d> & -> & <o,d,s>, <o,d,t> \\
 & <y,d> & -> & <y,d,s>, <y,d,t>\\
 & <b,m> &  & NA
\end{tabular}

\bigbreak 
FOIL evaluates the addition of a literal L to a rule R by 
$$
\text{FOIL\_Gain}(L,R) 
= t(\text{Info}(R_0) - \text{Info}(R_1)) 
= t(\text{log}_2 \frac{p_1}{p_1 + n_1} - \text{log}_2 \frac{p_0}{p_0 + n_0})
$$

where

$p_0$ = the number of positive tuples covered by $R_0$ = 4

$n_0$ = the number of negative tuples covered by $R_0$ = 3

$p_1$ = the number of positive tuples covered by $R_0 \land L$ = 8

$n_1$ = the number of negative tuples covered by $R_0 \land L$ = 4

$t$ = the number of positive of tuples of R also covered by $R_0 \land L$ = $p_1 - p_0$  = 4

\bigbreak 
Therefore, 
$$
\text{FOIL\_Gain}(L,R) 
= 4(\text{log}_2 \frac{2}{3} - \text{log}_2 \frac{4}{7}) \approx .88957
$$


\end{document}